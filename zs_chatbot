from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import json
from tqdm import tqdm
from datetime import datetime
import os
import sys
import re

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)

timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
result_dir = "outputs"
os.makedirs(result_dir, exist_ok=True)
json_filename = f"{result_dir}/results_chatbot_zscal_{timestamp}.json"
log_filename = f"{result_dir}/log_chatbot_zscal_{timestamp}.txt"

class Tee:
    def __init__(self, *files):
        self.files = files
    def write(self, obj):
        for f in self.files:
            f.write(obj)
            f.flush()
    def flush(self):
        for f in self.files:
            f.flush()

log_file = open(log_filename, "w")
sys.stdout = Tee(sys.stdout, log_file)

model_name = "meta-llama/Llama-2-7b-chat-hf"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16 if device == "cuda" else torch.float32
).to(device).eval()

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

with open("CAL/data/chatbot/sampled.jsonl", "r") as f:
    lines = [json.loads(line) for line in f]

def normalize_label(label):
    """Normalize labels to match expected format"""
    if isinstance(label, str):
        label = label.upper().strip()
        if label in ['MODEL_A', 'A']:
            return 'A'
        elif label in ['MODEL_B', 'B']:
            return 'B'
        elif label in ['TIE', 'BOTH', 'EQUAL', 'C']:
            return 'TIE'
    return label

def make_prompt(example):
    """Simple and effective prompt format from ZS-CAL"""
    return f"""Which response better answers the question?

Question: {example['q']}

Response A: {example['a']}

Response B: {example['b']}

Better response (A/B/TIE):"""

def truncate_text(text, max_length=1000):
    """Truncate text if too long"""
    if len(text) > max_length:
        return text[:max_length] + "..."
    return text

def make_prompt_with_truncation(example):
    """Create prompt with automatic truncation if needed"""
    
    question = truncate_text(example['q'], 300)
    response_a = truncate_text(example['a'], 800)
    response_b = truncate_text(example['b'], 800)
    
    return f"""Which response better answers the question?

Question: {question}

Response A: {response_a}

Response B: {response_b}

Better response (A/B/TIE):"""

def extract_prediction(text):
    """Extract prediction using the same method as ZS-CAL"""
    text = text.strip().upper()
    
    
    if text.startswith('A'):
        return 'A'
    elif text.startswith('B'):
        return 'B'
    elif text.startswith('TIE'):
        return 'TIE'
    
    
    matches = re.findall(r'\b(A|B|TIE)\b', text)
    if matches:
        return matches[-1]  
    
    return "INVALID"

results = []
correct = 0
total = 0
invalid_count = 0

print(f"Processing {min(50, len(lines))} examples...")

for i, example in enumerate(tqdm(lines)):
    try:
        
        prompt_text = make_prompt(example)
        
        
        inputs = tokenizer(
            prompt_text,
            return_tensors="pt",
            truncation=True,
            max_length=2048,
            padding=True
        )
        
        
        if inputs['input_ids'].shape[1] >= 2000:
            prompt_text = make_prompt_with_truncation(example)
            inputs = tokenizer(
                prompt_text,
                return_tensors="pt",
                truncation=True,
                max_length=2048,
                padding=True
            )
        
        inputs = inputs.to(device)
        
        with torch.no_grad():
            output = model.generate(
                **inputs,
                max_new_tokens=10,  # Keep short like ZS-CAL
                do_sample=True,
                temperature=0.1,
                top_p=0.95,
                pad_token_id=tokenizer.eos_token_id
            )
        
        
        generated_tokens = output[0][inputs['input_ids'].shape[1]:]
        response_text = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()
        
        pred = extract_prediction(response_text)
        true_label = normalize_label(example.get("judge", "Unknown"))
        
        
        if pred in ["A", "B", "TIE"]:
            total += 1
            if pred == true_label:
                correct += 1
        else:
            invalid_count += 1
        
        results.append({
            "index": example.get("index", i),
            "user_question": example["q"][:200] + "..." if len(example["q"]) > 200 else example["q"],
            "response_a": example["a"][:200] + "..." if len(example["a"]) > 200 else example["a"],
            "response_b": example["b"][:200] + "..." if len(example["b"]) > 200 else example["b"],
            "true_judge": true_label,
            "predicted_judge": pred,
            "raw_output": response_text,
            "prompt_length": inputs['input_ids'].shape[1]
        })
        
        print(f"\n[{i}] Prediction: {pred} | True: {true_label} | Match: {pred == true_label}")
        print(f"Raw Output: '{response_text}'")
        
    except Exception as e:
        print(f"Error at index {i}: {e}")
        continue

accuracy = correct / total if total > 0 else 0
print(f"\nZero-Shot CAL Chatbot Accuracy: {accuracy:.2%} ({correct}/{total})")
print(f"Invalid predictions: {invalid_count}")

pred_counts = {}
true_counts = {}
for result in results:
    pred = result["predicted_judge"]
    true_label = result["true_judge"]
    pred_counts[pred] = pred_counts.get(pred, 0) + 1
    true_counts[true_label] = true_counts.get(true_label, 0) + 1

print(f"\nPrediction distribution: {pred_counts}")
print(f"True label distribution: {true_counts}")


print(f"\nDetailed Results:")
for label in ['A', 'B', 'TIE']:
    correct_for_label = sum(1 for r in results if r["true_judge"] == label and r["predicted_judge"] == label)
    total_for_label = sum(1 for r in results if r["true_judge"] == label)
    if total_for_label > 0:
        acc = correct_for_label / total_for_label
        print(f"  {label}: {acc:.2%} ({correct_for_label}/{total_for_label})")

output_data = {
    "method": "Zero-Shot CAL Chatbot",
    "accuracy": accuracy,
    "correct_predictions": correct,
    "total_valid_predictions": total,
    "invalid_predictions": invalid_count,
    "total_examples": len(results),
    "prediction_distribution": pred_counts,
    "true_label_distribution": true_counts,
    "timestamp": timestamp,
    "results": results
}

with open(json_filename, "w") as f:
    json.dump(output_data, f, indent=2)

print(f"\nResults saved to:")
print(f" JSON: {json_filename}")
print(f" Log: {log_filename}")

log_file.close()
