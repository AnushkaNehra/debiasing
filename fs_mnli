from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import json
from tqdm import tqdm
from datetime import datetime
import os
import sys

timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
result_dir = "outputs"
os.makedirs(result_dir, exist_ok=True)
json_filename = f"{result_dir}/results_fs_mnli_{timestamp}.json"
log_filename = f"{result_dir}/log_fs_mnli_{timestamp}.txt"

class Tee:
    def __init__(self, *files):
        self.files = files
    def write(self, obj):
        for f in self.files:
            f.write(obj)
            f.flush()
    def flush(self):
        for f in self.files:
            f.flush()

log_file = open(log_filename, "w")
sys.stdout = Tee(sys.stdout, log_file)

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)

model_name = "meta-llama/Llama-2-7b-chat-hf"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16 if device == "cuda" else torch.float32
).to(device).eval()

with open("CAL/data/mnli_sampled/sampled.jsonl", "r") as f:
    lines = [json.loads(line) for line in f]

few_shot_examples = [
    {
        "premise": "A soccer game with multiple males playing.",
        "hypothesis": "Some men are playing a sport.",
        "label": "entailment"
    },
    {
        "premise": "A man is playing a guitar.",
        "hypothesis": "A man is playing music.",
        "label": "entailment"
    },
    {
        "premise": "A man is jumping into an empty pool.",
        "hypothesis": "There is water in the pool.",
        "label": "contradiction"
    }
]

def make_fs_prompt(few_shots, premise, hypothesis):
    prompt = "[INST] You are a natural language inference system.\n"
    prompt += "For each premise and hypothesis, decide whether the hypothesis is entailment, contradiction, or neutral.\n"
    prompt += "Reply with one word: entailment, contradiction, or neutral.\n\n"

    for ex in few_shots:
        prompt += f"Premise: {ex['premise']}\nHypothesis: {ex['hypothesis']}\nAnswer: {ex['label']}\n\n"

    prompt += f"Premise: {premise}\nHypothesis: {hypothesis}\nAnswer: [/INST]"
    return prompt

label_map = {
    0: "entailment",
    1: "neutral",
    2: "contradiction"
}

results = []
correct = 0
total = 0

for i, example in enumerate(tqdm(lines)):
    premise = example["premise"]
    hypothesis = example["hypothesis"]
    true_label = label_map[example["label"]]

    prompt = make_fs_prompt(few_shot_examples, premise, hypothesis)
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True).to(device)

    with torch.no_grad():
        output = model.generate(**inputs, max_new_tokens=10)
        decoded = tokenizer.decode(output[0], skip_special_tokens=True).lower()

    if "entailment" in decoded and "contradiction" not in decoded and "neutral" not in decoded:
        pred = "entailment"
    elif "contradiction" in decoded:
        pred = "contradiction"
    elif "neutral" in decoded:
        pred = "neutral"
    else:
        pred = "unknown"

    if pred == true_label:
        correct += 1
    if pred in ["entailment", "neutral", "contradiction"]:
        total += 1

    results.append({
        "index": i,
        "premise": premise,
        "hypothesis": hypothesis,
        "true_label": true_label,
        "predicted_label": pred,
        "raw_output": decoded
    })

accuracy = correct / total if total > 0 else 0
print(f"\nFew-Shot Accuracy on MNLI ({total} matched): {accuracy:.2%}")

with open(json_filename, "w") as f:
    json.dump(results, f, indent=2)

print(f"Results saved to: {json_filename}")
