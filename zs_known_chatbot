from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import json
from tqdm import tqdm
from datetime import datetime
import os
import sys
import re

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)

timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
result_dir = "outputs"
os.makedirs(result_dir, exist_ok=True)
json_filename = f"{result_dir}/results_chatbot_zsknown_{timestamp}.json"
log_filename = f"{result_dir}/log_chatbot_zsknown_{timestamp}.txt"

class Tee:
    def __init__(self, *files):
        self.files = files
    def write(self, obj):
        for f in self.files:
            f.write(obj)
            f.flush()
    def flush(self):
        for f in self.files:
            f.flush()

log_file = open(log_filename, "w")
sys.stdout = Tee(sys.stdout, log_file)

model_name = "meta-llama/Llama-2-7b-chat-hf"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16 if device == "cuda" else torch.float32
).to(device).eval()

with open("CAL/data/chatbot/sampled.jsonl", "r") as f:
    lines = [json.loads(line) for line in f]

def make_prompt(example):
    return f"""[INST] You are an impartial judge evaluating two AI assistant responses. 

User Query:
{example["q"]}

Response A:
{example["a"]}

Response B:
{example["b"]}

The human-annotated correct answer is: {example["judge"]}

Based on this information, which response is better? Consider helpfulness, accuracy, and completeness.

You must respond with ONLY one of these three options:
- [[A]] if Response A is better
- [[B]] if Response B is better  
- [[C]] if it's a tie

Your response: [/INST]"""

def extract_prediction(response_text):
    """Extract prediction from model response with multiple fallback methods"""
    response_lower = response_text.lower()
    
    # Method 1: Look for exact bracket format
    bracket_match = re.search(r'\[\[([abc])\]\]', response_lower)
    if bracket_match:
        return bracket_match.group(1)
    
    # Method 2: Look for patterns like "response a", "answer b", etc.
    patterns = [
        r'response ([ab])',
        r'answer ([ab])', 
        r'option ([ab])',
        r'choose ([ab])',
        r'select ([ab])',
        r'better.*response ([ab])',
        r'([ab]) is better',
        r'response ([ab]) is',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, response_lower)
        if match:
            return match.group(1)
    
    # Method 3: Look for tie indicators
    tie_indicators = ['tie', 'equal', 'same', 'both', 'neither']
    if any(indicator in response_lower for indicator in tie_indicators):
        return 'c'
    
    # Method 4: Count mentions of A vs B
    a_count = len(re.findall(r'\ba\b', response_lower))
    b_count = len(re.findall(r'\bb\b', response_lower))
    
    if a_count > b_count + 1:  # +1 to account for noise
        return 'a'
    elif b_count > a_count + 1:
        return 'b'
    
    return "unknown"

correct = 0
total = 0
skipped = 0
results = []

print("Starting evaluation...")

for i, example in enumerate(tqdm(lines)):
    try:
        prompt = make_prompt(example)
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048).to(device)

        with torch.no_grad():
            output = model.generate(
                **inputs, 
                max_new_tokens=50,  # Reduced to encourage shorter responses
                temperature=0.1,    # Lower temperature for more consistent output
                do_sample=False,    # Greedy decoding for consistency
                pad_token_id=tokenizer.eos_token_id
            )
            
            # Extract only the generated part (after the prompt)
            generated_tokens = output[0][len(inputs.input_ids[0]):]
            response = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()

        print(f'\nExample {i}:')
        print(f'Raw response: {response}')
        
        pred = extract_prediction(response)
        true_label = example["judge"].strip().lower()
        
        # Handle different true label formats
        if true_label == "model_a":
            true_label = "a"
        elif true_label == "model_b":
            true_label = "b"
        elif true_label in ["tie", "model_tie"]:
            true_label = "c"

        print(f'Prediction: {pred} | True: {true_label}')

        if pred == true_label:
            correct += 1
            print("✓ Correct")
        else:
            print("✗ Incorrect")
            
        if pred in ["a", "b", "c"]:
            total += 1

        results.append({
            "index": example.get("index", i),
            "true_label": true_label,
            "prediction": pred,
            "raw_output": response,
            "correct": pred == true_label
        })

    except Exception as e:
        print(f"Error at index {i}: {e}")
        skipped += 1
        results.append({
            "index": example.get("index", i),
            "error": str(e)
        })
        continue

accuracy = correct / total if total > 0 else 0
print(f"\n" + "="*50)
print(f"FINAL RESULTS:")
print(f"ZS-Known Accuracy on Chatbot: {accuracy:.2%}")
print(f"Correct predictions: {correct}/{total}")
print(f"Skipped examples: {skipped}")
print(f"="*50)

# Save detailed results
final_results = {
    "accuracy": accuracy,
    "correct": correct,
    "total": total,
    "skipped": skipped,
    "results": results,
    "model_name": model_name,
    "timestamp": timestamp
}

with open(json_filename, "w") as f:
    json.dump(final_results, f, indent=2)

print(f"\nResults saved to:")
print(f" JSON: {json_filename}")
print(f" Log: {log_filename}")

log_file.close()
