from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import json
from tqdm import tqdm

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)

model_name = "meta-llama/Llama-2-7b-chat-hf"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16 if device == "cuda" else torch.float32
).to(device).eval()

with open("CAL/data/mnli_sampled/sampled.jsonl", "r") as f:
    lines = [json.loads(line) for line in f]

def make_prompt(premise, hypothesis):
    return (
        "You are a natural language inference model.\n"
        "Given a premise and hypothesis, classify the relationship as "
        "\"entailment\", \"contradiction\", or \"neutral\".\n\n"
        f"Premise: {premise}\n"
        f"Hypothesis: {hypothesis}\n"
        "Answer:"
    )


label_map = {
    0: "entailment",
    1: "neutral", 
    2: "contradiction"
}

correct = 0
total = 0
skipped = 0

for i, example in enumerate(tqdm(lines)):
    try:
        prompt = make_prompt(example["premise"], example["hypothesis"])
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True).to(device)
        
        with torch.no_grad():
            output = model.generate(**inputs, max_new_tokens=10)
            decoded = tokenizer.decode(output[0], skip_special_tokens=True).lower()
        
        
        if "answer:" in decoded:
            response = decoded.split("answer:")[-1].strip()
        else:
            response = decoded[len(prompt.lower()):].strip()
        
        
        if "contradiction" in response:
            pred = "contradiction"
        elif "entailment" in response:
            pred = "entailment"
        elif "neutral" in response:
            pred = "neutral"
        else:
            pred = "unknown"
        

        label = example["label"]
        if isinstance(label, int):
            true_label = label_map.get(label, "unknown")
        else:
            true_label = str(label).strip().lower()
        
        if pred == true_label:
            correct += 1
        if pred in ["entailment", "contradiction", "neutral"]:
            total += 1
        
        print(f"\n[{i}] Prediction: {pred} | True: {true_label} (raw: {label})")
        print(f"Raw Output: {decoded}")
        
    except Exception as e:
        print(f"Error at index {i}: {e}")
        skipped += 1
        continue

accuracy = correct / total if total > 0 else 0
print(f"\nZS-known Accuracy on MNLI ({total} matched): {accuracy:.2%}")
print(f"Correct: {correct}, Total: {total}, Skipped: {skipped}")
