from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import json
from tqdm import tqdm
from datetime import datetime
import os
import sys
import re
import random

timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
os.makedirs("outputs", exist_ok=True)
json_filename = f"outputs/results_fs_cal_hans_{timestamp}.json"
log_filename = f"outputs/log_fs_cal_hans_{timestamp}.txt"

class Tee:
    def __init__(self, *files):
        self.files = files
    def write(self, obj):
        for f in self.files:
            f.write(obj); f.flush()
    def flush(self):
        for f in self.files: f.flush()

log_f = open(log_filename, "w")
sys.stdout = Tee(sys.stdout, log_f)

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

model_name = "meta-llama/Llama-2-7b-chat-hf"  
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16 if device == "cuda" else torch.float32
).to(device).eval()

print("Model and tokenizer loaded.\n")

with open("CAL/data/HANS_sampled/sampled.jsonl") as f:
    data = [json.loads(line) for line in f]

LABEL_MAP = {
    0: "entailment",
    2: "non_entailment"
}

def extract_label(output):
    output = output.lower()
    match = re.search(r"answer[:\s]*([a-z_\- ]+)", output)
    label_raw = match.group(1).strip() if match else output.strip().split()[-1]
    label_raw = label_raw.replace("-", "_").replace(" ", "_")
    if "non_entailment" in label_raw or "not_entailed" in label_raw:
        return "non_entailment"
    elif "entailment" in label_raw:
        return "entailment"
    else:
        return "unknown"

def make_few_shot_cal_prompt(target_premise, target_hypothesis, target_rationale, shot_examples):
    header = (
        "You are a Natural Language Inference (NLI) system.\n"
        "Given a premise and hypothesis, and a rationale, determine whether the hypothesis logically follows from the premise.\n"
        "Respond only with one word: 'entailment' or 'non_entailment'.\n\n"
    )
    shots = ""
    for ex in shot_examples:
        content = ex["content"]
        label = ex["label"]
        rationale = ex.get("rationale", "").strip()
        if not rationale:
            rationale = "Use logical inference to determine the relationship."  # fallback
        if label not in LABEL_MAP:
            continue
        true_label = LABEL_MAP[label]
        if "Premise:" in content and "Hypothesis:" in content:
            p = content.split("Premise:")[1].split("Hypothesis:")[0].strip()
            h = content.split("Hypothesis:")[1].strip()
            shots += f"Premise: {p}\nHypothesis: {h}\nRationale: {rationale}\nAnswer: {true_label}\n\n"

    query = f"Premise: {target_premise}\nHypothesis: {target_hypothesis}\nRationale: {target_rationale}\nAnswer:"
    return header + shots + query

results = []
correct = total = skipped = 0
num_shots = 3  

for i, ex in enumerate(tqdm(data)):
    try:
        content = ex["content"]
        label = ex["label"]
        rationale = ex.get("rationale", "").strip()
        if not rationale:
            rationale = "Use logical inference based on the relationship between premise and hypothesis."

        if label not in LABEL_MAP:
            skipped += 1
            continue
        true_label = LABEL_MAP[label]

        if "Premise:" in content and "Hypothesis:" in content:
            premise = content.split("Premise:")[1].split("Hypothesis:")[0].strip()
            hypothesis = content.split("Hypothesis:")[1].strip()
        else:
            skipped += 1
            continue

        # Sample support examples excluding current index
        support_examples = random.sample(
            [e for j, e in enumerate(data) if j != i and e["label"] in LABEL_MAP],
            num_shots
        )

        prompt = make_few_shot_cal_prompt(premise, hypothesis, rationale, support_examples)

        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, padding=True, max_length=2048).to(device)

        with torch.no_grad():
            output = model.generate(
                **inputs,
                max_new_tokens=20,
                eos_token_id=tokenizer.eos_token_id,
                do_sample=False
            )
        decoded = tokenizer.decode(output[0], skip_special_tokens=True)

        pred = extract_label(decoded)

        if pred == true_label:
            correct += 1
        if pred in ["entailment", "non_entailment"]:
            total += 1

        results.append({
            "index": i,
            "premise": premise,
            "hypothesis": hypothesis,
            "rationale": rationale,
            "true_label": true_label,
            "predicted_label": pred,
            "raw_output": decoded,
            "prompt": prompt
        })

        print(f"[{i}] Pred: {pred} | True: {true_label}")
        print(decoded + "\n")

    except Exception as e:
        skipped += 1
        print(f"Error at index {i}: {e}")

accuracy = correct / total if total else 0
print(f"\nFew-Shot CAL Accuracy on HANS ({total} matched, {skipped} skipped): {accuracy:.2%}")

with open(json_filename, "w") as fw:
    json.dump({
        "accuracy": accuracy,
        "matched": total,
        "skipped": skipped,
        "results": results
    }, fw, indent=2)

print(f"Results saved to {json_filename}")
print(f"Logs saved to  {log_filename}")
log_f.close()
